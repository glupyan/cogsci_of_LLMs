<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cognitive Science of Large Language Models on Psych 711: Cognitive Science of Large Language Models. Fall, 2025</title>
    <link>/</link>
    <description>Recent content in Cognitive Science of Large Language Models on Psych 711: Cognitive Science of Large Language Models. Fall, 2025</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Aug 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Logistics</title>
      <link>/logistics/</link>
      <pubDate>Tue, 26 Aug 2025 00:00:00 +0000</pubDate>
      <guid>/logistics/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Course:&lt;/strong&gt; Psych 711: Cognitive Science of Large Language Models&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Instructor:&lt;/strong&gt; Prof. Gary Lupyan. [&lt;a href=&#34;mailto:lupyan@wisc.edu&#34;&gt;Email&lt;/a&gt;]. [&lt;a href=&#34;http://sapir.psych.wisc.edu&#34;&gt;Lab site&lt;/a&gt;]&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Class Hours:&lt;/strong&gt; Wednesdays 9:00am-11:30am, Brogden 338&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Office hours:&lt;/strong&gt; Mondays 1pm-2pm, Brogden 526&lt;/p&gt;&#xA;&lt;h2 id=&#34;where-to-access-the-readings&#34;&gt;Where to access the readings&lt;/h2&gt;&#xA;&lt;p&gt;I&amp;rsquo;ve uploaded all the readings to Google Drive. Feel free to download the whole thing to a local folder. If there are changes to the syllabus, the Google Drive folder will be automatically updated.&lt;/p&gt;&#xA;&lt;h2 id=&#34;where-to-submit-what&#34;&gt;Where to submit what&lt;/h2&gt;&#xA;&lt;p&gt;Submit essays, reading responses, and final projects using the &lt;a href=&#34;https://canvas.wisc.edu/courses/474927/&#34;&gt;class Canvas site&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Resources</title>
      <link>/resources/</link>
      <pubDate>Tue, 26 Aug 2025 00:00:00 +0000</pubDate>
      <guid>/resources/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;Blogdown book: &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;https://bookdown.org/yihui/blogdown/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Hugo: &lt;a href=&#34;https://gohugo.io/&#34;&gt;https://gohugo.io/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Netlify docs: &lt;a href=&#34;https://docs.netlify.com/&#34;&gt;https://docs.netlify.com/&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Add additional course-specific tools here.&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>/schedule/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/schedule/</guid>
      <description>&lt;h1 id=&#34;class-schedule&#34;&gt;Class Schedule&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;&#xA;&lt;p&gt;I recommend reading the papers in the order in which they are listed.&lt;/p&gt;&#xA;&lt;h1 id=&#34;week-1-wednesday-september-3-2025&#34;&gt;Week 1 (Wednesday, September 3, 2025)&lt;/h1&gt;&#xA;&lt;h3 id=&#34;what-is-learnable&#34;&gt;What is learnable?&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Readings (in class):&lt;/strong&gt; &lt;a href=&#34;javascript:void(0)&#34; class=&#34;cite-pop&#34; role=&#34;button&#34; tabindex=&#34;0&#34; data-ref=&#34;Sutton, R. (2019). The Bitter Lesson. &amp;lt;em&amp;gt;Cs&amp;lt;/em&amp;gt;. &amp;lt;a href=&amp;quot;https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf&amp;quot; target=&amp;quot;_blank&amp;quot; rel=&amp;quot;noopener&amp;quot;&amp;gt;https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf&amp;lt;/a&amp;gt;&#34; data-plain=&#34;Sutton, R. (2019). The Bitter Lesson. Cs. https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf&#34;&gt;Sutton (2019)&lt;/a&gt;; &lt;a href=&#34;javascript:void(0)&#34; class=&#34;cite-pop&#34; role=&#34;button&#34; tabindex=&#34;0&#34; data-ref=&#34;Aguera y Arcas, B. (2025). What Is Intelligence?. &amp;lt;a href=&amp;quot;https://whatisintelligence.antikythera.org/introduction/&amp;quot; target=&amp;quot;_blank&amp;quot; rel=&amp;quot;noopener&amp;quot;&amp;gt;https://whatisintelligence.antikythera.org/introduction/&amp;lt;/a&amp;gt;&#34; data-plain=&#34;Aguera y Arcas, B. (2025). What Is Intelligence?. https://whatisintelligence.antikythera.org/introduction/&#34;&gt;Aguera y Arcas (2025)&lt;/a&gt;;&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;Related resources&lt;/em&gt;: &lt;a href=&#34;javascript:void(0)&#34; class=&#34;cite-pop&#34; role=&#34;button&#34; tabindex=&#34;0&#34; data-ref=&#34;Halevy, A., Norvig, P., &amp;amp;amp; Pereira, F. (2009). The Unreasonable Effectiveness of Data. &amp;lt;em&amp;gt;IEEE Intelligent Systems&amp;lt;/em&amp;gt;, &amp;lt;em&amp;gt;24&amp;lt;/em&amp;gt;(2), 8–12. &amp;lt;a href=&amp;quot;https://doi.org/10.1109/MIS.2009.36&amp;quot; target=&amp;quot;_blank&amp;quot; rel=&amp;quot;noopener&amp;quot;&amp;gt;https://doi.org/10.1109/MIS.2009.36&amp;lt;/a&amp;gt;&#34; data-plain=&#34;Halevy, A., Norvig, P., &amp;amp; Pereira, F. (2009). The Unreasonable Effectiveness of Data. IEEE Intelligent Systems, 24(2), 8–12. https://doi.org/10.1109/MIS.2009.36&#34;&gt;Halevy et al. (2009)&lt;/a&gt;; &lt;a href=&#34;youtube.com/watch?v=R7qy2BY6mTk&amp;amp;list=PL2xTeGtUb-8B94jdWGT-chu4ucI7oEe_x&amp;amp;index=32&amp;amp;pp=iAQB&#34;&gt;talk by Alyosha Efros&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;week-2-wednesday-september-10-2025&#34;&gt;Week 2 (Wednesday, September 10, 2025)&lt;/h1&gt;&#xA;&lt;h3 id=&#34;how-do-llms-work&#34;&gt;How do LLMs work?&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Readings:&lt;/strong&gt; &lt;a href=&#34;javascript:void(0)&#34; class=&#34;cite-pop&#34; role=&#34;button&#34; tabindex=&#34;0&#34; data-ref=&#34;Rumelhart, D. E. (1989). The Architecture of Mind: A Connectionist Approach. &amp;lt;em&amp;gt;Foundations of Cognitive Science&amp;lt;/em&amp;gt;, 133–159.&#34; data-plain=&#34;Rumelhart, D. E. (1989). The Architecture of Mind: A Connectionist Approach. Foundations of Cognitive Science, 133–159.&#34;&gt;Rumelhart (1989)&lt;/a&gt;; &lt;a href=&#34;javascript:void(0)&#34; class=&#34;cite-pop&#34; role=&#34;button&#34; tabindex=&#34;0&#34; data-ref=&#34;Lee, T. B. (2025). Large Language Models, Explained with a Minimum of Math and Jargon. &amp;lt;em&amp;gt;Understandingai&amp;lt;/em&amp;gt;. &amp;lt;a href=&amp;quot;https://www.understandingai.org/p/large-language-models-explained-with&amp;quot; target=&amp;quot;_blank&amp;quot; rel=&amp;quot;noopener&amp;quot;&amp;gt;https://www.understandingai.org/p/large-language-models-explained-with&amp;lt;/a&amp;gt;&#34; data-plain=&#34;Lee, T. B. (2025). Large Language Models, Explained with a Minimum of Math and Jargon. Understandingai. https://www.understandingai.org/p/large-language-models-explained-with&#34;&gt;Lee (2025)&lt;/a&gt;; Either &lt;a href=&#34;javascript:void(0)&#34; class=&#34;cite-pop&#34; role=&#34;button&#34; tabindex=&#34;0&#34; data-ref=&#34;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp;amp;amp; Polosukhin, I. (2017). Attention Is All You Need. &amp;lt;em&amp;gt;arXiv&amp;lt;/em&amp;gt; 1706.03762. &amp;lt;a href=&amp;quot;https://doi.org/10.48550/arXiv.1706.03762&amp;quot; target=&amp;quot;_blank&amp;quot; rel=&amp;quot;noopener&amp;quot;&amp;gt;https://doi.org/10.48550/arXiv.1706.03762&amp;lt;/a&amp;gt;&#34; data-plain=&#34;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp;amp; Polosukhin, I. (2017). Attention Is All You Need. arXiv 1706.03762. https://doi.org/10.48550/arXiv.1706.03762&#34;&gt;Vaswani et al. (2017)&lt;/a&gt; or &lt;a href=&#34;javascript:void(0)&#34; class=&#34;cite-pop&#34; role=&#34;button&#34; tabindex=&#34;0&#34; data-ref=&#34;Phuong, M. &amp;amp;amp; Hutter, M. (2022). Formal Algorithms for Transformers. &amp;lt;em&amp;gt;arXiv&amp;lt;/em&amp;gt; 2207.09238. &amp;lt;a href=&amp;quot;https://doi.org/10.48550/arXiv.2207.09238&amp;quot; target=&amp;quot;_blank&amp;quot; rel=&amp;quot;noopener&amp;quot;&amp;gt;https://doi.org/10.48550/arXiv.2207.09238&amp;lt;/a&amp;gt;&#34; data-plain=&#34;Phuong, M. &amp;amp; Hutter, M. (2022). Formal Algorithms for Transformers. arXiv 2207.09238. https://doi.org/10.48550/arXiv.2207.09238&#34;&gt;Phuong &amp;amp; Hutter (2022)&lt;/a&gt; .&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Other resources for understanding transformers (very useful; strongly recommended!)&lt;/strong&gt;:&#xA;&lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/?utm_source=chatgpt.com&#34;&gt;The illustrated transformer&lt;/a&gt;; &lt;a href=&#34;https://www.youtube.com/watch?v=wjZofJX0v4M&amp;amp;ab_channel=3Blue1Brown&#34;&gt;Transformers, the tech behind LLMs video&lt;/a&gt; and the &lt;a href=&#34;https://www.youtube.com/watch?v=eMlx5fFNoYc&amp;amp;ab_channel=3Blue1Brown&#34;&gt;next chapter focusing on attention&lt;/a&gt;;&lt;/li&gt;&#xA;&lt;li&gt;Extra: &lt;a href=&#34;javascript:void(0)&#34; class=&#34;cite-pop&#34; role=&#34;button&#34; tabindex=&#34;0&#34; data-ref=&#34;McCoy, R. T., Yao, S., Friedman, D., Hardy, M. D., &amp;amp;amp; Griffiths, T. L. (2024). Embers of Autoregression Show How Large Language Models Are Shaped by the Problem They Are Trained to Solve. &amp;lt;em&amp;gt;Proceedings of the National Academy of Sciences&amp;lt;/em&amp;gt;, &amp;lt;em&amp;gt;121&amp;lt;/em&amp;gt;(41), e2322420121. &amp;lt;a href=&amp;quot;https://doi.org/10.1073/pnas.2322420121&amp;quot; target=&amp;quot;_blank&amp;quot; rel=&amp;quot;noopener&amp;quot;&amp;gt;https://doi.org/10.1073/pnas.2322420121&amp;lt;/a&amp;gt;&#34; data-plain=&#34;McCoy, R. T., Yao, S., Friedman, D., Hardy, M. D., &amp;amp; Griffiths, T. L. (2024). Embers of Autoregression Show How Large Language Models Are Shaped by the Problem They Are Trained to Solve. Proceedings of the National Academy of Sciences, 121(41), e2322420121. https://doi.org/10.1073/pnas.2322420121&#34;&gt;McCoy et al. (2024)&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;week-3-wednesday-september-17-2025&#34;&gt;Week 3 (Wednesday, September 17, 2025)&lt;/h1&gt;&#xA;&lt;h3 id=&#34;learning-language-from-language&#34;&gt;Learning language from language&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Special guest&lt;/strong&gt; - Steven Piantadosi&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>/schedule_bib/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/schedule_bib/</guid>
      <description>&lt;h1 id=&#34;class-schedule&#34;&gt;Class Schedule&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;&#xA;&lt;p&gt;I recommend reading the papers in the order in which they are listed.&lt;/p&gt;&#xA;&lt;h1 id=&#34;r-advdatewed-1&#34;&gt;&lt;code&gt;r advdate(wed, 1)&lt;/code&gt;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;what-is-learnable&#34;&gt;What is learnable?&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Readings (in class):&lt;/strong&gt; @suttonBitterLesson2019; @WhatIntelligenceAntikythera;&lt;/li&gt;&#xA;&lt;li&gt;&lt;em&gt;Related resources&lt;/em&gt;: @halevyUnreasonableEffectivenessData2009; &lt;a href=&#34;youtube.com/watch?v=R7qy2BY6mTk&amp;amp;list=PL2xTeGtUb-8B94jdWGT-chu4ucI7oEe_x&amp;amp;index=32&amp;amp;pp=iAQB&#34;&gt;talk by Alyosha Efros&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;r-advdatewed-2&#34;&gt;&lt;code&gt;r advdate(wed, 2)&lt;/code&gt;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;how-do-llms-work&#34;&gt;How do LLMs work?&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Readings:&lt;/strong&gt; @rumelhartArchitectureMindConnectionist1989; @leeLargeLanguageModels2025; Either @vaswaniAttentionAllYou2023 or @phuongFormalAlgorithmsTransformers2022 .&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Other resources for understanding transformers (very useful; strongly recommended!)&lt;/strong&gt;:&#xA;&lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/?utm_source=chatgpt.com&#34;&gt;The illustrated transformer&lt;/a&gt;; &lt;a href=&#34;https://www.youtube.com/watch?v=wjZofJX0v4M&amp;amp;ab_channel=3Blue1Brown&#34;&gt;Transformers, the tech behind LLMs video&lt;/a&gt; and the &lt;a href=&#34;https://www.youtube.com/watch?v=eMlx5fFNoYc&amp;amp;ab_channel=3Blue1Brown&#34;&gt;next chapter focusing on attention&lt;/a&gt;;&lt;/li&gt;&#xA;&lt;li&gt;Extra: @mccoyEmbersAutoregressionShow2024&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;r-advdatewed-3&#34;&gt;&lt;code&gt;r advdate(wed, 3)&lt;/code&gt;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;learning-language-from-language&#34;&gt;Learning language from language&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Special guest&lt;/strong&gt; - Steven Piantadosi&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
