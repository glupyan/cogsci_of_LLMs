@online{aguerayarcasLargeLanguageModels2022,
  title = {Do Large Language Models Understand Us?},
  author = {Agüera y Arcas, B.},
  date = {2022-02-16T06:41:22},
  url = {https://medium.com/@blaisea/do-large-language-models-understand-us-6f881d6d8e75},
  urldate = {2024-02-08},
  abstract = {LLMs have a great deal to teach us about the nature of language, understanding, intelligence, sociality, and personhood.},
  langid = {english},
  organization = {Medium}
}

@article{banerjeeChildrensArithmeticSkills2025,
  title = {Children’s Arithmetic Skills Do Not Transfer between Applied and Academic Mathematics},
  author = {Banerjee, Abhijit V. and Bhattacharjee, Swati and Chattopadhyay, Raghabendra and Duflo, Esther and Ganimian, Alejandro J. and Rajah, Kailash and Spelke, Elizabeth S.},
  date = {2025-03},
  journaltitle = {Nature},
  volume = {639},
  number = {8055},
  pages = {673--681},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-08502-w},
  url = {https://www.nature.com/articles/s41586-024-08502-w},
  urldate = {2025-08-08},
  abstract = {Many children from low-income backgrounds worldwide fail to master school mathematics1; however, some children extensively use mental arithmetic outside school2,3. Here we surveyed children in Kolkata and Delhi, India, who work in markets (n\,=\,1,436), to investigate whether maths skills acquired in real-world settings transfer to the classroom and vice versa. Nearly all these children used complex arithmetic calculations effectively at work. They were also proficient in solving hypothetical market maths problems and verbal maths problems that were anchored to concrete contexts. However, they were unable to solve arithmetic problems of equal or lesser complexity when presented in the abstract format typically used in school. The children’s performance in market maths problems was not explained by memorization, access to help, reduced stress with more familiar formats or high incentives for correct performance. By contrast, children with no market-selling experience (n\,=\,471), enrolled in nearby schools, showed the opposite pattern. These children performed more accurately on simple abstract problems, but only 1\% could correctly answer an applied market maths problem that more than one third of working children solved (β\,=\,0.35, s.e.m.\,=\,0.03; 95\% confidence interval\,=\,0.30–0.40, P\,{$<$}\,0.001). School children used highly inefficient written calculations, could not combine different operations and arrived at answers too slowly to be useful in real-life or in higher maths. These findings highlight the importance of educational curricula that bridge the gap between intuitive and formal maths.},
  langid = {english},
  keywords = {Economics,Education,Human behaviour},
  file = {/Users/glupyan/Zotero/storage/6G6HELIG/Banerjee et al. - 2025 - Children’s arithmetic skills do not transfer between applied and academic mathematics.pdf}
}

@article{barsalou_perceptual_1999,
  title = {Perceptual Symbol Systems},
  author = {Barsalou, L.W.},
  date = {1999-08},
  journaltitle = {The Behavioral and Brain Sciences},
  shortjournal = {Behav Brain Sci},
  volume = {22},
  number = {4},
  eprint = {11301525},
  eprinttype = {pubmed},
  pages = {577-609; discussion 610-660},
  issn = {0140-525X},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/11301525},
  urldate = {2010-02-22},
  abstract = {Prior to the twentieth century, theories of knowledge were inherently perceptual. Since then, developments in logic, statistics, and programming languages have inspired amodal theories that rest on principles fundamentally different from those underlying perception. In addition, perceptual approaches have become widely viewed as untenable because they are assumed to implement recording systems, not conceptual systems. A perceptual theory of knowledge is developed here in the context of current cognitive science and neuroscience. During perceptual experience, association areas in the brain capture bottom-up patterns of activation in sensory-motor areas. Later, in a top-down manner, association areas partially reactivate sensory-motor areas to implement perceptual symbols. The storage and reactivation of perceptual symbols operates at the level of perceptual components--not at the level of holistic perceptual experiences. Through the use of selective attention, schematic representations of perceptual components are extracted from experience and stored in memory (e.g., individual memories of green, purr, hot). As memories of the same component become organized around a common frame, they implement a simulator that produces limitless simulations of the component (e.g., simulations of purr). Not only do such simulators develop for aspects of sensory experience, they also develop for aspects of proprioception (e.g., lift, run) and introspection (e.g., compare, memory, happy, hungry). Once established, these simulators implement a basic conceptual system that represents types, supports categorization, and produces categorical inferences. These simulators further support productivity, propositions, and abstract concepts, thereby implementing a fully functional conceptual system. Productivity results from integrating simulators combinatorially and recursively to produce complex simulations. Propositions result from binding simulators to perceived individuals to represent type-token relations. Abstract concepts are grounded in complex simulations of combined physical and introspective events. Thus, a perceptual theory of knowledge can implement a fully functional conceptual system while avoiding problems associated with amodal symbol systems. Implications for cognition, neuroscience, evolution, development, and artificial intelligence are explored.},
  keywords = {Cognition,Concept Formation,Humans,Knowledge,Memory,Models Psychological,Perception}
}

@article{barsalou_staying_2016,
  title = {On {{Staying Grounded}} and {{Avoiding Quixotic Dead Ends}}},
  author = {Barsalou, L.W.},
  date = {2016-04-25},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  pages = {1--21},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-016-1028-3},
  url = {http://link.springer.com/article/10.3758/s13423-016-1028-3},
  urldate = {2016-07-20},
  abstract = {The 15 articles in this special issue on The Representation of Concepts illustrate the rich variety of theoretical positions and supporting research that characterize the area. Although much agreement exists among contributors, much disagreement exists as well, especially about the roles of grounding and abstraction in conceptual processing. I first review theoretical approaches raised in these articles that I believe are Quixotic dead ends, namely, approaches that are principled and inspired but likely to fail. In the process, I review various theories of amodal symbols, their distortions of grounded theories, and fallacies in the evidence used to support them. Incorporating further contributions across articles, I then sketch a theoretical approach that I believe is likely to be successful, which includes grounding, abstraction, flexibility, explaining classic conceptual phenomena, and making contact with real-world situations. This account further proposes that (1) a key element of grounding is neural reuse, (2) abstraction takes the forms of multimodal compression, distilled abstraction, and distributed linguistic representation (but not amodal symbols), and (3) flexible context-dependent representations are a hallmark of conceptual processing.},
  langid = {english},
  file = {/Users/glupyan/Zotero/storage/MD5WT8W7/Barsalou_2016_On Staying Grounded and Avoiding Quixotic Dead Ends.pdf;/Users/glupyan/Zotero/storage/I6UZ6VWR/10.html}
}

@article{binzMetalearnedModelsCognition2024,
  title = {Meta-Learned Models of Cognition},
  author = {Binz, Marcel and Dasgupta, Ishita and Jagadish, Akshay K. and Botvinick, Matthew and Wang, Jane X. and Schulz, Eric},
  date = {2024-01},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {47},
  pages = {e147},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X23003266},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/metalearned-models-of-cognition/F95059E07AE6E82AE56C4164A5384A18},
  urldate = {2025-08-25},
  abstract = {Psychologists and neuroscientists extensively rely on computational models for studying and analyzing the human mind. Traditionally, such computational models have been hand-designed by expert researchers. Two prominent examples are cognitive architectures and Bayesian models of cognition. Although the former requires the specification of a fixed set of computational structures and a definition of how these structures interact with each other, the latter necessitates the commitment to a particular prior and a likelihood function that – in combination with Bayes' rule – determine the model's behavior. In recent years, a new framework has established itself as a promising tool for building models of human cognition: the framework of meta-learning. In contrast to the previously mentioned model classes, meta-learned models acquire their inductive biases from experience, that is, by repeatedly interacting with an environment. However, a coherent research program around meta-learned models of cognition is still missing to date. The purpose of this article is to synthesize previous work in this field and establish such a research program. We accomplish this by pointing out that meta-learning can be used to construct Bayes-optimal learning algorithms, allowing us to draw strong connections to the rational analysis of cognition. We then discuss several advantages of the meta-learning framework over traditional methods and reexamine prior work in the context of these new insights.},
  langid = {english},
  keywords = {Bayesian inference,cognitive modeling,meta-learning,neural networks,rational analysis},
  file = {/Users/glupyan/Zotero/storage/PU6YMDKW/Binz et al. - 2024 - Meta-learned models of cognition.pdf}
}

@article{boleda_distributional_2020,
  title = {Distributional {{Semantics}} and {{Linguistic Theory}}},
  author = {Boleda, Gemma},
  date = {2020},
  journaltitle = {Annual Review of Linguistics},
  volume = {Accepted},
  eprint = {1905.01896},
  eprinttype = {arXiv},
  doi = {10.1146/annurev-linguistics-011619-030303},
  url = {http://arxiv.org/abs/1905.01896},
  urldate = {2019-07-26},
  abstract = {Distributional semantics provides multi-dimensional, graded, empirically induced word representations that successfully capture many aspects of meaning in natural languages, as shown in a large body of work in computational linguistics; yet, its impact in theoretical linguistics has so far been limited. This survey provides a critical discussion of the literature on distributional semantics, with an emphasis on methods and results that are of relevance for theoretical linguistics, in three areas: semantic change, polysemy and composition, and the grammar-semantics interface (specifically, the interface of semantics with syntax and with derivational morphology). The survey aims at fostering a greater cross-fertilization of theoretical and computational approaches to language, as a means to advance our collective knowledge of how it works.},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/glupyan/Zotero/storage/EME6RNXD/Boleda - 2019 - Distributional Semantics and Linguistic Theory.pdf;/Users/glupyan/Zotero/storage/75RABGUM/1905.html}
}

@incollection{bowerExperimentalMethodsCognitive1989,
  title = {Experimental Methods in Cognitive Science},
  booktitle = {Foundations of Cognitive Science},
  author = {Bower, Gordon H. and Clapper, John P.},
  date = {1989},
  pages = {245--300},
  publisher = {The MIT Press},
  location = {Cambridge, MA, US},
  abstract = {naturalistic observation / correlational studies / controlled experiments / measuring experimental effects / isolating causal effects / coordinating theory with observables / experiments on human cognitive processes / characterizing psychological processes / analyzing representational types / additive factors method /dual tasks / signal detection theory  argued that intuitive instrospection is often a weak, uninformative, even biased, measuring instrument for rapid, nonconscious cognitive processes  we presented a few general methods used in experimental studies of cognition and indicated how analogical theories can be placed in correspondence to observable, experimental events / we then reviewed some specific techniques used in studies of memory and language processing, trying in most cases to indicate a few substantive issues addressed by them (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  isbn = {978-0-262-16112-1 978-0-262-66086-0},
  keywords = {Cognitive Processes,Experimental Methods,Language,Learning,Memory},
  file = {/Users/glupyan/Zotero/storage/YP72FICL/1990-97026-007.html}
}

@incollection{casasanto_all_2014,
  title = {All {{Concepts}} Are {{Ad Hoc Concepts}}},
  booktitle = {Concepts: {{New Directions}}},
  author = {Casasanto, Daniel and Lupyan, G.},
  editor = {Margolis, E. and Laurence, S.},
  date = {2014},
  pages = {543--566},
  publisher = {MIT Press},
  location = {Cambridge}
}

@online{chalmersDoesThoughtRequire2024,
  title = {Does {{Thought Require Sensory Grounding}}? {{From Pure Thinkers}} to {{Large Language Models}}},
  shorttitle = {Does {{Thought Require Sensory Grounding}}?},
  author = {Chalmers, David J.},
  date = {2024-08-18},
  eprint = {2408.09605},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.09605},
  url = {http://arxiv.org/abs/2408.09605},
  urldate = {2025-08-19},
  abstract = {Does the capacity to think require the capacity to sense? A lively debate on this topic runs throughout the history of philosophy and now animates discussions of artificial intelligence. I argue that in principle, there can be pure thinkers: thinkers that lack the capacity to sense altogether. I also argue for significant limitations in just what sort of thought is possible in the absence of the capacity to sense. Regarding AI, I do not argue directly that large language models can think or understand, but I rebut one important argument (the argument from sensory grounding) that they cannot. I also use recent results regarding language models to address the question of whether or how sensory grounding enhances cognitive capacities.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/glupyan/Zotero/storage/4KAUGHL2/Chalmers - 2024 - Does Thought Require Sensory Grounding From Pure Thinkers to Large Language Models.pdf;/Users/glupyan/Zotero/storage/7CUTR7XN/2408.html}
}

@online{cholletMeasureIntelligence2019,
  title = {On the {{Measure}} of {{Intelligence}}},
  author = {Chollet, François},
  date = {2019-11-25},
  eprint = {1911.01547},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1911.01547},
  url = {http://arxiv.org/abs/1911.01547},
  urldate = {2025-08-25},
  abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/glupyan/Zotero/storage/AS7K6JE4/Chollet - 2019 - On the Measure of Intelligence.pdf;/Users/glupyan/Zotero/storage/NG58VQYS/1911.html}
}

@article{elman_finding_1990,
  title = {Finding {{Structure}} in {{Time}}},
  author = {Elman, J.L.},
  date = {1990-04},
  journaltitle = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  url = {ISI:A1990DK92500001},
  keywords = {ENGLISH}
}

@article{godduLLMsDontKnow2024,
  title = {{{LLMs}} Don’t Know Anything: Reply to {{Yildirim}} and {{Paul}}},
  shorttitle = {{{LLMs}} Don’t Know Anything},
  author = {Goddu, Mariel K. and Noë, Alva and Thompson, Evan},
  date = {2024-11-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {28},
  number = {11},
  eprint = {39304393},
  eprinttype = {pubmed},
  pages = {963--964},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2024.06.008},
  url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(24)00168-2},
  urldate = {2025-08-19},
  langid = {english},
  file = {/Users/glupyan/Zotero/storage/DIWJ75EE/Goddu et al. - 2024 - LLMs don’t know anything reply to Yildirim and Paul.pdf}
}

@article{gordon_numerical_2004,
  title = {Numerical Cognition without Words: {{Evidence}} from {{Amazonia}}},
  author = {Gordon, P.},
  date = {2004-10-15},
  journaltitle = {Science},
  volume = {306},
  number = {5695},
  pages = {496--499},
  abstract = {Members of the Piraha tribe use a "one-two-many" system of counting. I ask whether speakers of this innumerate language can appreciate larger numerosities without the benefit of words to encode them. This addresses the classic Whorfian question about whether language can determine thought. Results of numerical tasks with varying cognitive demands show that numerical cognition is clearly affected by the lack of a counting system in the language. Performance with quantities greater than three was remarkably poor, but showed a constant coefficient of variation, which is suggestive of an analog estimation process},
  keywords = {COGNITION,COUNT,DISCRIMINATION,ENGLISH,FOUNDATIONS,INFANTS,KNOWLEDGE,LANGUAGE,MAGNITUDES,number,ORIGINS,PERFORMANCE,REPRESENTATIONS,SPEAKERS,SYSTEM,TASK,TIME,WORD,WORDS}
}

@online{griffithsWhitherSymbolsEra2025,
  title = {Whither Symbols in the Era of Advanced Neural Networks?},
  author = {Griffiths, Thomas L. and Lake, Brenden M. and McCoy, R. Thomas and Pavlick, Ellie and Webb, Taylor W.},
  date = {2025-08-07},
  eprint = {2508.05776},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2508.05776},
  url = {http://arxiv.org/abs/2508.05776},
  urldate = {2025-08-25},
  abstract = {Some of the strongest evidence that human minds should be thought about in terms of symbolic systems has been the way they combine ideas, produce novelty, and learn quickly. We argue that modern neural networks -- and the artificial intelligence systems built upon them -- exhibit similar abilities. This undermines the argument that the cognitive processes and representations used by human minds are symbolic, although the fact that these neural networks are typically trained on data generated by symbolic systems illustrates that such systems play an important role in characterizing the abstract problems that human minds have to solve. This argument leads us to offer a new agenda for research on the symbolic basis of human thought.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/glupyan/Zotero/storage/Y7RANIJQ/Griffiths et al. - 2025 - Whither symbols in the era of advanced neural networks.pdf;/Users/glupyan/Zotero/storage/T46IWBHH/2508.html}
}

@article{halevyUnreasonableEffectivenessData2009,
  title = {The {{Unreasonable Effectiveness}} of {{Data}}},
  author = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
  date = {2009-03},
  journaltitle = {IEEE Intelligent Systems},
  volume = {24},
  number = {2},
  pages = {8--12},
  issn = {1941-1294},
  doi = {10.1109/MIS.2009.36},
  url = {https://ieeexplore.ieee.org/document/4804817},
  urldate = {2025-08-20},
  abstract = {Problems that involve interacting with humans, such as natural language understanding, have not proven to be solvable by concise, neat formulas like F = ma. Instead, the best approach appears to be to embrace the complexity of the domain and address it by harnessing the power of data: if other humans engage in the tasks and generate large amounts of unlabeled, noisy data, new algorithms can be used to build high-quality models from the data.},
  keywords = {Broadcasting,Data mining,Frequency estimation,Humans,machine learning,Machine learning,Natural language processing,Semantic Web,Speech recognition,Tagging,very large data bases,Videos,Web pages},
  file = {/Users/glupyan/Zotero/storage/EBTJS4YH/4804817.html}
}

@article{jonasCouldNeuroscientistUnderstand2017,
  title = {Could a {{Neuroscientist Understand}} a {{Microprocessor}}?},
  author = {Jonas, Eric and Kording, Konrad Paul},
  date = {2017-01-12},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {13},
  number = {1},
  pages = {e1005268},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005268},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005268},
  urldate = {2025-08-21},
  abstract = {There is a popular belief in neuroscience that we are primarily data limited, and that producing large, multimodal, and complex datasets will, with the help of advanced data analysis algorithms, lead to fundamental insights into the way the brain processes information. These datasets do not yet exist, and if they did we would have no way of evaluating whether or not the algorithmically-generated insights were sufficient or even correct. To address this, here we take a classical microprocessor as a model organism, and use our ability to perform arbitrary experiments on it to see if popular data analysis methods from neuroscience can elucidate the way it processes information. Microprocessors are among those artificial information processing systems that are both complex and that we understand at all levels, from the overall logical flow, via logical gates, to the dynamics of transistors. We show that the approaches reveal interesting structure in the data but do not meaningfully describe the hierarchy of information processing in the microprocessor. This suggests current analytic approaches in neuroscience may fall short of producing meaningful understanding of neural systems, regardless of the amount of data. Additionally, we argue for scientists using complex non-linear dynamical systems with known ground truth, such as the microprocessor as a validation platform for time-series and structure discovery methods.},
  langid = {english},
  keywords = {Behavior,Behavioral neuroscience,Computational neuroscience,Connectomics,Microprocessors,Neuronal tuning,Neurons,Neuroscience},
  file = {/Users/glupyan/Zotero/storage/UMJWK5B6/Jonas and Kording - 2017 - Could a Neuroscientist Understand a Microprocessor.pdf}
}

@article{lakeHumanlikeSystematicGeneralization2023,
  title = {Human-like Systematic Generalization through a Meta-Learning Neural Network},
  author = {Lake, Brenden M. and Baroni, Marco},
  date = {2023-11},
  journaltitle = {Nature},
  volume = {623},
  number = {7985},
  pages = {115--121},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06668-3},
  url = {https://www.nature.com/articles/s41586-023-06668-3},
  urldate = {2025-08-25},
  abstract = {The power of human language and thought arises from systematic compositionality—the algebraic ability to understand and produce novel combinations from known components. Fodor and Pylyshyn1 famously argued that artificial neural networks lack this capacity and are therefore not viable models of the mind. Neural networks have advanced considerably in the years since, yet the systematicity challenge persists. Here we successfully address Fodor and Pylyshyn’s challenge by providing evidence that neural networks can achieve human-like systematicity when optimized for their compositional skills. To do so, we introduce the meta-learning for compositionality (MLC) approach for guiding training through a dynamic stream of compositional tasks. To compare humans and machines, we conducted human behavioural experiments using an~instruction learning paradigm. After considering seven different models, we found that, in contrast to perfectly systematic but rigid probabilistic symbolic models, and perfectly flexible but unsystematic neural networks, only MLC achieves both the systematicity and flexibility needed for human-like generalization. MLC also advances the compositional skills of machine learning systems in several systematic generalization benchmarks. Our results show how a standard neural network architecture, optimized for its compositional skills, can mimic human systematic generalization in a head-to-head comparison.},
  langid = {english},
  keywords = {Computer science,Human behaviour},
  file = {/Users/glupyan/Zotero/storage/9APGNL3R/Lake and Baroni - 2023 - Human-like systematic generalization through a meta-learning neural network.pdf}
}

@article{lampinenLanguageModelsHumans2024,
  title = {Language Models, like Humans, Show Content Effects on Reasoning Tasks},
  author = {Lampinen, Andrew K. and Dasgupta, Ishita and Chan, Stephanie C. Y. and Sheahan, Hannah R. and Creswell, Antonia and Kumaran, Dharshan and McClelland, James L. and Hill, Felix},
  date = {2024-07},
  journaltitle = {PNAS nexus},
  shortjournal = {PNAS Nexus},
  volume = {3},
  number = {7},
  eprint = {39015546},
  eprinttype = {pubmed},
  pages = {pgae233},
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgae233},
  abstract = {reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks but exhibit many imperfections. However, human abstract reasoning is also imperfect. Human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns are central to debates about the fundamental nature of human intelligence. Here, we investigate whether language models-whose prior expectations capture some aspects of human knowledge-similarly mix content into their answers to logic problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art LMs, as well as humans, and find that the LMs reflect many of the same qualitative human patterns on these tasks-like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected in accuracy patterns, and in some lower-level features like the relationship between LM confidence over possible answers and human response times. However, in some cases the humans and models behave differently-particularly on the Wason task, where humans perform much worse than large models, and exhibit a distinct error pattern. Our findings have implications for understanding possible contributors to these human cognitive effects, as well as the factors that influence language model performance.},
  langid = {english},
  pmcid = {PMC11250216},
  keywords = {cognitive science,content effects,language models,logic,reasoning},
  file = {/Users/glupyan/Zotero/storage/HDQ2HGFK/Lampinen et al. - 2024 - Language models, like humans, show content effects on reasoning tasks.pdf}
}

@online{leeLargeLanguageModels2025,
  title = {Large Language Models, Explained with a Minimum of Math and Jargon},
  author = {Lee, Timothy B.},
  date = {2025-05-19},
  url = {https://www.understandingai.org/p/large-language-models-explained-with},
  urldate = {2025-08-19},
  abstract = {Want to really understand how large language models work? Here’s a gentle primer.},
  langid = {english},
  file = {/Users/glupyan/Zotero/storage/MA54Q297/large-language-models-explained-with.html}
}

@online{leggCollectionDefinitionsIntelligence2007,
  title = {A {{Collection}} of {{Definitions}} of {{Intelligence}}},
  author = {Legg, Shane and Hutter, Marcus},
  date = {2007-06-25},
  eprint = {0706.3639},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.0706.3639},
  url = {http://arxiv.org/abs/0706.3639},
  urldate = {2025-08-25},
  abstract = {This paper is a survey of a large number of informal definitions of ``intelligence'' that the authors have collected over the years. Naturally, compiling a complete list would be impossible as many definitions of intelligence are buried deep inside articles and books. Nevertheless, the 70-odd definitions presented here are, to the authors' knowledge, the largest and most well referenced collection there is.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/glupyan/Zotero/storage/MYNYCEM2/Legg and Hutter - 2007 - A Collection of Definitions of Intelligence.pdf;/Users/glupyan/Zotero/storage/SY97Q5TA/0706.html}
}

@online{leggUniversalIntelligenceDefinition2007,
  title = {Universal {{Intelligence}}: {{A Definition}} of {{Machine Intelligence}}},
  shorttitle = {Universal {{Intelligence}}},
  author = {Legg, Shane and Hutter, Marcus},
  date = {2007-12-20},
  eprint = {0712.3329},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.0712.3329},
  url = {http://arxiv.org/abs/0712.3329},
  urldate = {2025-08-25},
  abstract = {A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/glupyan/Zotero/storage/RXMUQQZI/Legg and Hutter - 2007 - Universal Intelligence A Definition of Machine Intelligence.pdf;/Users/glupyan/Zotero/storage/PTSUXBTC/0712.html}
}

@article{leslie2008generative,
  title = {The Generative Basis of Natural Number Concepts},
  author = {Leslie, Alan M and Gelman, Rochel and Gallistel, C.R.},
  date = {2008},
  journaltitle = {Trends in cognitive sciences},
  volume = {12},
  number = {6},
  pages = {213--218},
  publisher = {Elsevier}
}

@online{lindsey+BiologyLargeLanguage,
  title = {On the {{Biology}} of a {{Large Language Model}}},
  author = {Lindsey, Jack and Gurnee, Wes and Ameisen, Emmanuel and Chen, Brian and Pearce, Adam and Turner, Nicholas L. and Citro, Craig and Abrahams, David and Carter, Shan and Hosmer, Basil and Marcus, Jonathan and Sklar, Michael and Templeton, Adly and Bricken, Trenton and McDougall, Callum and Cunningham, Hoagy and Henighan, Thomas and Jermyn, Adam and Jones, Andy and Persic, Andrew and Qi, Zhenyi and Thompson, T. Ben and Zimmerman, Sam and Rivoire, Kelley and Conerly, Thomas and Olah, Chris and Batson, J.},
  date = {2025},
  url = {https://transformer-circuits.pub/2025/attribution-graphs/biology.html},
  urldate = {2025-08-21},
  abstract = {We investigate the internal mechanisms used by Claude 3.5 Haiku — Anthropic's lightweight production model — in a variety of contexts, using our circuit tracing methodology.},
  langid = {english},
  organization = {Transformer Circuits},
  file = {/Users/glupyan/Zotero/storage/CYKUYXA6/biology.html}
}

@article{lupyan_words-as-mappings_2017,
  title = {From Words-as-Mappings to Words-as-Cues: The Role of Language in Semantic Knowledge},
  shorttitle = {From Words-as-Mappings to Words-as-Cues},
  author = {Lupyan, G. and Lewis, M.},
  date = {2017-12-05},
  journaltitle = {Language, Cognition and Neuroscience},
  volume = {34},
  number = {10},
  pages = {1319--1337},
  issn = {2327-3798},
  doi = {10.1080/23273798.2017.1404114},
  url = {https://doi.org/10.1080/23273798.2017.1404114},
  urldate = {2019-01-26},
  abstract = {Semantic knowledge (or semantic memory) is knowledge we have about the world. For example, we know that knives are typically sharp, made of metal, and that they are tools used for cutting. To what kinds of experiences do we owe such knowledge? Most work has stressed the role of direct sensory and motor experiences. Another kind of experience, considerably less well understood, is our experience with language. We review two ways of thinking about the relationship between language and semantic knowledge: (i) language as mapping onto independently-acquired concepts, and (ii) language as a set of cues to meaning. We highlight some problems with the words-as-mappings view, and argue in favour of the words-as-cues alternative. We then review some surprising ways that language impacts semantic knowledge, and discuss how distributional semantics models can help us better understand its role. We argue that language has an abstracting effect on knowledge, helping to go beyond concrete experiences which are more characteristic of perception and action. We conclude by describing several promising directions for future research.},
  keywords = {Concepts,knowledge,language,semantics,words},
  file = {/Users/glupyan/Zotero/storage/PA7PYM4K/Lupyan and Lewis - 2017 - From words-as-mappings to words-as-cues the role .pdf;/Users/glupyan/Zotero/storage/2A6PMRC3/23273798.2017.html}
}

@article{mccoyEmbersAutoregressionShow2024,
  title = {Embers of Autoregression Show How Large Language Models Are Shaped by the Problem They Are Trained to Solve},
  author = {McCoy, R. Thomas and Yao, Shunyu and Friedman, Dan and Hardy, Mathew D. and Griffiths, Thomas L.},
  date = {2024-10-08},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {121},
  number = {41},
  pages = {e2322420121},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2322420121},
  url = {https://www.pnas.org/doi/10.1073/pnas.2322420121},
  urldate = {2025-08-05},
  abstract = {The widespread adoption of large language models (LLMs) makes it important to recognize their strengths and limitations. We argue that to develop a holistic understanding of these systems, we must consider the problem that they were trained to solve: next-word prediction over Internet text. By recognizing the pressures that this task exerts, we can make predictions about the strategies that LLMs will adopt, allowing us to reason about when they will succeed or fail. Using this approach—which we call the teleological approach—we identify three factors that we hypothesize will influence LLM accuracy: the probability of the task to be performed, the probability of the target output, and the probability of the provided input. To test our predictions, we evaluate five LLMs (GPT-3.5, GPT-4, Claude 3, Llama 3, and Gemini 1.0) on 11 tasks, and we find robust evidence that LLMs are influenced by probability in the hypothesized ways. Many of the experiments reveal surprising failure modes. For instance, GPT-4’s accuracy at decoding a simple cipher is 51\% when the output is a high-probability sentence but only 13\% when it is low-probability, even though this task is a deterministic one for which probability should not matter. These results show that AI practitioners should be careful about using LLMs in low-probability situations. More broadly, we conclude that we should not evaluate LLMs as if they are humans but should instead treat them as a distinct type of system—one that has been shaped by its own particular set of pressures.},
  file = {/Users/glupyan/Zotero/storage/D5CYNIA4/McCoy et al. - 2024 - Embers of autoregression show how large language models are shaped by the problem they are trained t.pdf}
}

@article{mitchell_debate_2023,
  title = {The Debate over Understanding in {{AI}}’s Large Language Models},
  author = {Mitchell, Melanie and Krakauer, David C.},
  date = {2023-03-28},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {13},
  pages = {e2215907120},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2215907120},
  url = {https://www.pnas.org/doi/10.1073/pnas.2215907120},
  urldate = {2023-11-06},
  abstract = {We survey a current, heated debate in the artificial intelligence (AI) research community on whether large pretrained language models can be said to understand language—and the physical and social situations language encodes—in any humanlike sense. We describe arguments that have been made for and against such understanding and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that an extended science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition.},
  file = {/Users/glupyan/Zotero/storage/PM3U5EFU/Mitchell and Krakauer - 2023 - The debate over understanding in AI’s large langua.pdf}
}

@online{mollo_vector_2023,
  title = {The {{Vector Grounding Problem}}},
  author = {Mollo, Dimitri Coelho and Millière, Raphaël},
  date = {2023-04-03},
  eprint = {2304.01481},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.01481},
  url = {http://arxiv.org/abs/2304.01481},
  urldate = {2023-11-06},
  abstract = {The remarkable performance of large language models (LLMs) on complex linguistic tasks has sparked a lively debate on the nature of their capabilities. Unlike humans, these models learn language exclusively from textual data, without direct interaction with the real world. Nevertheless, they can generate seemingly meaningful text about a wide range of topics. This impressive accomplishment has rekindled interest in the classical 'Symbol Grounding Problem,' which questioned whether the internal representations and outputs of classical symbolic AI systems could possess intrinsic meaning. Unlike these systems, modern LLMs are artificial neural networks that compute over vectors rather than symbols. However, an analogous problem arises for such systems, which we dub the Vector Grounding Problem. This paper has two primary objectives. First, we differentiate various ways in which internal representations can be grounded in biological or artificial systems, identifying five distinct notions discussed in the literature: referential, sensorimotor, relational, communicative, and epistemic grounding. Unfortunately, these notions of grounding are often conflated. We clarify the differences between them, and argue that referential grounding is the one that lies at the heart of the Vector Grounding Problem. Second, drawing on theories of representational content in philosophy and cognitive science, we propose that certain LLMs, particularly those fine-tuned with Reinforcement Learning from Human Feedback (RLHF), possess the necessary features to overcome the Vector Grounding Problem, as they stand in the requisite causal-historical relations to the world that underpin intrinsic meaning. We also argue that, perhaps unexpectedly, multimodality and embodiment are neither necessary nor sufficient conditions for referential grounding in artificial systems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/glupyan/Zotero/storage/VAACDMK3/Mollo and Millière - 2023 - The Vector Grounding Problem.pdf;/Users/glupyan/Zotero/storage/6PX9Y2PP/2304.html}
}

@online{nandaProgressMeasuresGrokking2023,
  title = {Progress Measures for Grokking via Mechanistic Interpretability},
  author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  date = {2023-10-19},
  eprint = {2301.05217},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.05217},
  url = {http://arxiv.org/abs/2301.05217},
  urldate = {2025-08-25},
  abstract = {Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \textbackslash textit\{progress measures\} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/glupyan/Zotero/storage/TUKYDEFW/Nanda et al. - 2023 - Progress measures for grokking via mechanistic interpretability.pdf;/Users/glupyan/Zotero/storage/4R2TMX3F/2301.html}
}

@article{nickelsSingleCaseStudies2022,
  title = {Single Case Studies Are a Powerful Tool for Developing, Testing and Extending Theories},
  author = {Nickels, Lyndsey and Fischer-Baum, Simon and Best, Wendy},
  date = {2022-12},
  journaltitle = {Nature Reviews Psychology},
  shortjournal = {Nat Rev Psychol},
  volume = {1},
  number = {12},
  pages = {733--747},
  publisher = {Nature Publishing Group},
  issn = {2731-0574},
  doi = {10.1038/s44159-022-00127-y},
  url = {https://www.nature.com/articles/s44159-022-00127-y},
  urldate = {2025-08-19},
  abstract = {Psychology embraces a diverse range of methodologies. However, most rely on averaging group data to draw conclusions. In this Perspective, we argue that single case methodology is a valuable tool for developing and extending psychological theories. We stress the importance of single case and case series research, drawing on classic and contemporary cases in which cognitive and perceptual deficits provide insights into typical cognitive processes in domains such as memory, delusions, reading and face perception. We unpack the key features of single case methodology, describe its strengths, its value in adjudicating~between theories, and outline its benefits for a~better understanding of deficits and hence more appropriate interventions. The unique insights that single case studies have provided illustrate the value of in-depth investigation within an individual. Single case methodology has an important place in the psychologist’s toolkit and it should be valued as a primary research tool.},
  langid = {english},
  keywords = {Neurological disorders,Psychology}
}

@article{oshaughnessy2019cultural,
  title = {The Cultural Origins of Number},
  author = {O'Shaughnessy, David M. and Gibson, Edward and Piantadosi, Steven T.},
  date = {2021},
  journaltitle = {Psychological Review},
  url = {http://colala.berkeley.edu/papers/oshaughnessy2021cultural.pdf},
  tags = {development,crosscultural,number}
}

@article{oshaughnessyDiverseMathematicalKnowledge2023,
  title = {Diverse Mathematical Knowledge among Indigenous {{Amazonians}}},
  author = {O’Shaughnessy, David M. and Cruz Cordero, Tania and Mollica, Francis and Boni, Isabelle and Jara-Ettinger, Julian and Gibson, Edward and Piantadosi, Steven T.},
  date = {2023-08-29},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {35},
  pages = {e2215999120},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2215999120},
  url = {https://www.pnas.org/doi/10.1073/pnas.2215999120},
  urldate = {2025-08-25},
  abstract = {We investigate number and arithmetic learning among a Bolivian indigenous people, the Tsimane’, for whom formal schooling is comparatively recent in history and variable in both extent and consistency. We first present a large-scale meta-analysis on child number development involving over 800 Tsimane’ children. The results emphasize the impact of formal schooling: Children are only found to be full counters when they have attended school, suggesting the importance of cultural support for early mathematics. We then test especially remote Tsimane’ communities and document the development of specialized arithmetical knowledge in the absence of direct formal education. Specifically, we describe individuals who succeed on arithmetic problems involving the number five—which has a distinct role in the local economy—even though they do not succeed on some lower numbers. Some of these participants can perform multiplication with fives at greater accuracy than addition by one. These results highlight the importance of cultural factors in early mathematics and suggest that psychological theories of number where quantities are derived from lower numbers via repeated addition (e.g., a successor function) are unlikely to explain the diversity of human mathematical ability.},
  file = {/Users/glupyan/Zotero/storage/CEKQ4ECX/O’Shaughnessy et al. - 2023 - Diverse mathematical knowledge among indigenous Amazonians.pdf}
}

@article{pavlickSymbolsGroundingLarge2023,
  title = {Symbols and Grounding in Large Language Models},
  author = {Pavlick, Ellie},
  date = {2023-06-05},
  journaltitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {381},
  number = {2251},
  pages = {20220041},
  publisher = {Royal Society},
  doi = {10.1098/rsta.2022.0041},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2022.0041},
  urldate = {2025-08-19},
  abstract = {Large language models (LLMs) are one of the most impressive achievements of artificial intelligence in recent years. However, their relevance to the study~of language more broadly remains unclear. This article considers the potential of LLMs to serve as models of language understanding in humans. While debate on this question typically centres around models’ performance on challenging language understanding tasks, this article argues that the answer depends on models’ underlying competence, and thus that the focus of the debate should be on empirical work which seeks to characterize the representations and processing algorithms that underlie model behaviour. From this perspective, the article offers counterarguments to two commonly cited reasons why LLMs cannot serve as plausible models of language in humans: their lack of symbolic structure and their lack of grounding. For each, a case is made that recent empirical trends undermine the common assumptions about LLMs, and thus that it is premature to draw conclusions about LLMs’ ability (or lack thereof) to offer insights on human language representation and understanding. This article is part of a discussion meeting issue ‘Cognitive artificial intelligence’.},
  keywords = {cognitive science,language models,natural language processing},
  file = {/Users/glupyan/Zotero/storage/FR3YF7QN/Pavlick - 2023 - Symbols and grounding in large language models.pdf}
}

@online{phuongFormalAlgorithmsTransformers2022,
  title = {Formal {{Algorithms}} for {{Transformers}}},
  author = {Phuong, Mary and Hutter, Marcus},
  date = {2022-07-19},
  eprint = {2207.09238},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.09238},
  url = {http://arxiv.org/abs/2207.09238},
  urldate = {2025-08-26},
  abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/glupyan/Zotero/storage/DRUGYS95/Phuong and Hutter - 2022 - Formal Algorithms for Transformers.pdf;/Users/glupyan/Zotero/storage/4XMDLSRH/2207.html}
}

@online{piantadosi_meaning_2022,
  title = {Meaning without Reference in Large Language Models},
  author = {Piantadosi, Steven T. and Hill, Felix},
  date = {2022-08-12},
  eprint = {2208.02957},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.02957},
  url = {http://arxiv.org/abs/2208.02957},
  urldate = {2023-11-07},
  abstract = {The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings. Contrary to claims that LLMs possess no meaning whatsoever, we argue that they likely capture important aspects of meaning, and moreover work in a way that approximates a compelling account of human cognition in which meaning arises from conceptual role. Because conceptual role is defined by the relationships between internal representational states, meaning cannot be determined from a model's architecture, training data, or objective function, but only by examination of how its internal states relate to each other. This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/glupyan/Zotero/storage/N8MPTGB5/Piantadosi and Hill - 2022 - Meaning without reference in large language models.pdf;/Users/glupyan/Zotero/storage/YEERV6MP/2208.html}
}

@article{piantadosiModernLanguageModels2024,
  title = {Modern Language Models Refute {{Chomsky}}'s Approach to Language},
  author = {Piantadosi, Steven T.},
  date = {2024-07-05},
  journaltitle = {From fieldwork to linguistic theory},
  pages = {353--414},
  publisher = {Language Science Press},
  location = {Berlin},
  url = {https://zenodo.org/records/12665933},
  urldate = {2025-08-06},
  abstract = {Modern machine learning has subverted and bypassed the theoretical frameworkof Chomsky’s generative approach to linguistics, including its core claims to particular insights, principles, structures, and processes. I describe the sense in whichmodern language models implement genuine theories of language, and I highlightthe links between these models and approaches to linguistics that are based ongradient computations and memorized constructions. I also describe why thesemodels undermine strong claims for the innateness of language and respond toseveral critiques of large language models, including arguments that they can’t answer “why” questions and skepticism that they are informative about real life acquisition. Most notably, large language models have attained remarkable successat discovering grammar without using any of the methods that some in linguisticsinsisted were necessary for a science of language to progress.},
  isbn = {9783961104734},
  langid = {english}
}

@article{quilty-dunn_best_2022,
  title = {The {{Best Game}} in {{Town}}: {{The Re-Emergence}} of the {{Language}} of {{Thought Hypothesis Across}} the {{Cognitive Sciences}}},
  shorttitle = {The {{Best Game}} in {{Town}}},
  author = {Quilty-Dunn, Jake and Porot, Nicolas and Mandelbaum, Eric},
  date = {2022-12-06},
  journaltitle = {The Behavioral and Brain Sciences},
  shortjournal = {Behav Brain Sci},
  eprint = {36471543},
  eprinttype = {pubmed},
  pages = {1--55},
  issn = {1469-1825},
  doi = {10.1017/S0140525X22002849},
  abstract = {Mental representations remain the central posits of psychology after many decades of scrutiny. However, there is no consensus about the representational format(s) of biological cognition. This paper provides a survey of evidence from computational cognitive psychology, perceptual psychology, developmental psychology, comparative psychology, and social psychology, and concludes that one type of format that routinely crops up is the language of thought (LoT). We outline six core properties of LoTs: (i) discrete constituents; (ii) role-filler independence; (iii) predicate-argument structure; (iv) logical operators; (v) inferential promiscuity; and (vi) abstract content. These properties cluster together throughout cognitive science. Bayesian computational modeling, compositional features of object perception, complex infant and animal reasoning, and automatic, intuitive cognition in adults all implicate LoT-like structures. Instead of regarding LoT as a relic of the previous century, researchers in cognitive science and philosophy of mind must take seriously the explanatory breadth of LoT-based architectures. We grant that the mind may harbor many formats and architectures, including iconic and associative structures as well as deep-neural-network-like architectures. However, as computational/representational approaches to the mind continue to advance, classical compositional symbolic structures-i.e., LoTs-only prove more flexible and well-supported over time.},
  langid = {english},
  keywords = {animal cognition,automaticity,cognitive architecture,deep learning,dual-process theories,implicit attitudes,infant cognition,language of thought,object files,visual cognition},
  file = {/Users/glupyan/Zotero/storage/TG7VYG2V/Quilty-Dunn et al. - 2022 - The Best Game in Town The Re-Emergence of the Lan.pdf}
}

@incollection{rumelhartArchitectureMindConnectionist1989,
  title = {The Architecture of Mind: {{A}} Connectionist Approach},
  shorttitle = {The Architecture of Mind},
  booktitle = {Foundations of Cognitive Science},
  author = {Rumelhart, David E.},
  date = {1989},
  pages = {133--159},
  publisher = {The MIT Press},
  location = {Cambridge, MA, US},
  abstract = {brain-style computation / in this chapter I begin with a somewhat more formal sketch of the computational framework of connectionist models / I then follow with a general discussion of the kinds of computational problems that connectionist models seem best suited for / finally, I will briefly review the state of the art in connectionist modeling (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  isbn = {978-0-262-16112-1 978-0-262-66086-0},
  keywords = {Artificial Intelligence,Cognitive Processes,Models},
  file = {/Users/glupyan/Zotero/storage/3JCZSC8T/1990-97026-004.html}
}

@article{evans_dual-processing_2008,
  title = {Dual-{{Processing Accounts}} of {{Reasoning}}, {{Judgment}}, and {{Social Cognition}}},
  author = {Evans, Jonathan St. B. T.},
  date = {2008},
  journaltitle = {Annual Review of Psychology},
  volume = {59},
  number = {1},
  eprint = {18154502},
  eprinttype = {pubmed},
  pages = {255--278},
  doi = {10.1146/annurev.psych.59.103006.093629},
  url = {http://www.annualreviews.org/doi/abs/10.1146/annurev.psych.59.103006.093629},
  urldate = {2014-02-24},
  abstract = {This article reviews a diverse set of proposals for dual processing in higher cognition within largely disconnected literatures in cognitive and social psychology. All these theories have in common the distinction between cognitive processes that are fast, automatic, and unconscious and those that are slow, deliberative, and conscious. A number of authors have recently suggested that there may be two architecturally (and evolutionarily) distinct cognitive systems underlying these dual-process accounts. However, it emerges that (a) there are multiple kinds of implicit processes described by different theorists and (b) not all of the proposed attributes of the two kinds of processing can be sensibly mapped on to two systems as currently conceived. It is suggested that while some dual-process theories are concerned with parallel competing processes involving explicit and implicit knowledge systems, others are concerned with the influence of preconscious processes that contextualize and shape deliberative reasoning and decision-making.},
  keywords = {Decision-making,dual-process theory,reasoning,social cognition,Thinking}
}


@online{suttonBitterLesson2019,
  title = {The {{Bitter Lesson}}},
  author = {Sutton, R.},
  date = {2019},
  url = {https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf},
  urldate = {2025-08-18}
}

@article{szathmary_major_1995,
  title = {The Major Evolutionary Transitions},
  author = {Szathmáry, Eörs and Smith, John Maynard},
  date = {1995},
  journaltitle = {Nature},
  volume = {374},
  number = {6519},
  pages = {227--232},
  url = {http://joelvelasco.net/teaching/167win10/szathmary%20and%20maynard%20smith%2095-major%20evo%20transitions.pdf},
  urldate = {2015-03-19},
  file = {/Users/glupyan/Zotero/storage/V6A82FME/Szathmáry_Smith_1995_The major evolutionary transitions.pdf}
}

@article{van_der_maas_dynamical_2006,
  title = {A Dynamical Model of General Intelligence: The Positive Manifold of Intelligence by Mutualism},
  shorttitle = {A Dynamical Model of General Intelligence},
  author = {family=Maas, given=Han L. J., prefix=van der, useprefix=true and Dolan, Conor V. and Grasman, Raoul P. P. P. and Wicherts, Jelte M. and Huizenga, Hilde M. and Raijmakers, Maartje E. J.},
  date = {2006-10},
  journaltitle = {Psychological Review},
  shortjournal = {Psychol Rev},
  volume = {113},
  number = {4},
  eprint = {17014305},
  eprinttype = {pubmed},
  pages = {842--861},
  issn = {0033-295X},
  doi = {10.1037/0033-295X.113.4.842},
  abstract = {Scores on cognitive tasks used in intelligence tests correlate positively with each other, that is, they display a positive manifold of correlations. The positive manifold is often explained by positing a dominant latent variable, the g factor, associated with a single quantitative cognitive or biological process or capacity. In this article, a new explanation of the positive manifold based on a dynamical model is proposed, in which reciprocal causation or mutualism plays a central role. It is shown that the positive manifold emerges purely by positive beneficial interactions between cognitive processes during development. A single underlying g factor plays no role in the model. The model offers explanations of important findings in intelligence research, such as the hierarchical factor structure of intelligence, the low predictability of intelligence from early childhood performance, the integration/differentiation effect, the increase in heritability of g, and the Jensen effect, and is consistent with current explanations of the Flynn effect.},
  langid = {english},
  keywords = {Humans,Intelligence,Models Psychological,Psychology}
}

@online{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2025-08-19},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/glupyan/Zotero/storage/9AB4MIY9/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/Users/glupyan/Zotero/storage/XV3ENPNM/1706.html}
}

@article{wangConstructingMeaningLanguageforthcoming,
  title = {Constructing Meaning from Language: Visual Knowledge in People Born Blind and in Large Language Models},
  author = {Wang, Z and Akshi and Keil, S. and Kim, Judy S. and Bedny, Marina},
  year = {in press},
  journaltitle = {Annual Review of Linguistics}
}


@book{WhatIntelligenceAntikythera,
  title = {What Is {{Intelligence}}?},
  shorttitle = {What Is {{Intelligence}}?},
  author = {Agüera y Arcas, B.},
  date = {2025},
  url = {https://whatisintelligence.antikythera.org/introduction/},
  urldate = {2025-08-19},
  langid = {english},
  file = {/Users/glupyan/Zotero/storage/NJ2993R3/introduction.html}
}


@book{arcasWhatIntelligenceLessons2025,
  location = {Cambridge, {MA}, {USA}},
  title = {What Is Intelligence?: Lessons from {AI} About Evolution, Computing, and Minds},
  isbn = {978-0-262-04995-5},
  url = {https://mitpress.mit.edu/9780262049955/what-is-intelligence/},
  series = {Antikythera},
  shorttitle = {What Is Intelligence?},
  abstract = {What intelligence really is, and how {AI}’s emergence is a natural consequence of evolution.},
  pagetotal = {624},
  publisher = {{MIT} Press},
  author = {Agüera y Arcas, B.},
  editorb = {Bratton, Benjamin H.},
  editorbtype = {redactor},
  urldate = {2025-09-03},
  date = {2025-09-23},
  langid = {english},
  file = {Open Access:/Users/glupyan/Zotero/storage/KQMYKC5Y/what-is-intelligence.html:text/html},
}

@article{yarkoniGeneralizabilityCrisis2020,
  title = {The Generalizability Crisis},
  author = {Yarkoni, Tal},
  date = {2020-12-21},
  journaltitle = {The Behavioral and Brain Sciences},
  shortjournal = {Behav Brain Sci},
  volume = {45},
  eprint = {33342451},
  eprinttype = {pubmed},
  pages = {e1},
  issn = {1469-1825},
  doi = {10.1017/S0140525X20001685},
  abstract = {Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned - that is, that the two must refer to roughly the same set of hypothetical observations. Here, I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology - the linear mixed model - I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers' actual generalization intentions. I demonstrate that although the "random effect" formalism is used pervasively in psychology to model intersubject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false-positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that failure to take the alignment between verbal and statistical expressions seriously lies at the heart of many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
  langid = {english},
  pmcid = {PMC10681374},
  keywords = {Generalization,Humans,inference,Intention,philosophy of science,psychology,Psychology,random effects,statistics},
  file = {/Users/glupyan/Zotero/storage/KWY68JUW/Yarkoni - 2020 - The generalizability crisis.pdf}
}

@article{yildirimResponseGodduNew2024,
  title = {Response to {{Goddu}} et al.: New Ways of Characterizing and Acquiring Knowledge},
  shorttitle = {Response to {{Goddu}} et Al.},
  author = {Yildirim, Ilker and Paul, L. A.},
  date = {2024-11},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends Cogn Sci},
  volume = {28},
  number = {11},
  eprint = {39304392},
  eprinttype = {pubmed},
  pages = {965--966},
  issn = {1879-307X},
  doi = {10.1016/j.tics.2024.08.004},
  langid = {english}
}

@article{yildirimTaskStructuresWorld2024,
  title = {From Task Structures to World Models: What Do {{LLMs}} Know?},
  shorttitle = {From Task Structures to World Models},
  author = {Yildirim, Ilker and Paul, L. A.},
  date = {2024-05-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {28},
  number = {5},
  pages = {404--415},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2024.02.008},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661324000354},
  urldate = {2025-08-19},
  abstract = {In what sense does a large language model (LLM) have knowledge? We answer by granting LLMs ‘instrumental knowledge’: knowledge gained by using next-word generation as an instrument. We then ask how instrumental knowledge is related to the ordinary, ‘worldly knowledge’ exhibited by humans, and explore this question in terms of the degree to which instrumental knowledge can be said to incorporate the structured world models of cognitive science. We discuss ways LLMs could recover degrees of worldly knowledge and suggest that such recovery will be governed by an implicit, resource-rational tradeoff between world models and tasks. Our answer to this question extends beyond the capabilities of a particular AI system and challenges assumptions about the nature of knowledge and intelligence.},
  keywords = {instrumental knowledge,intelligence,large language models,resource rational,world models,worldly knowledge},
  file = {/Users/glupyan/Zotero/storage/73JB9QSI/Yildirim and Paul - 2024 - From task structures to world models what do LLMs know.pdf;/Users/glupyan/Zotero/storage/ZX6VX6L8/S1364661324000354.html}
}

